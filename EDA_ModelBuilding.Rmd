---
title: "EDA"
author: "Jia Yu Cheung"
date: "2023-03-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(factoextra)
library(tidyverse)
library(corrplot)
library(ggplot2)
library(psych) # Contains the winsor function and other helpful statistical tools
library(patchwork)
library(readr)
library(reshape2)
library(robustHD)
library(DescTools)
library(caTools)
library(gam)
library(nnet)
library(randomForest)
library(ISLR)
library(pROC)
library(mgcv)
library(caret)
library(segmented)
library(rpart)
library(rpart.plot)
library(glmnet)
library(class)
library(e1071)
library(gmodels) # CrossTable()
set.seed(580)
```

# Section 2: Model Building

# EDA: Feature Importance: Random Forest 
#(https://www.r-bloggers.com/2021/07/feature-importance-in-random-forest/)
#(https://www.listendata.com/2014/11/random-forest-with-r.html#id-b53c14)

```{r}
set.seed(580)

RFmodel <- randomForest(ADCDRSTG~., data = train_data) 

# Evaluate variable important
var_imp <- importance(RFmodel, type = 1) #How much model accuracy decreases if we leave out that variable
var_imp

# Plot the variable importance measures
varImpPlot(RFmodel,
           n.var=20,
           main = 'Random Forest: Variable Importance')

plot(RFmodel)
legend("topright", colnames(RFmodel$err.rate), col = 1:6, lty = 1, cex = 0.8)
which.min(RFmodel$err.rate)
```
```{r}
# Make predictions for test set
set.seed(580)
rfPreds = predict(RFmodel, test_data)

confusionTab_rf = table(Predicted = rfPreds, Actual = test_data$ADCDRSTG)
confusionTab_rf

acc_rf = round((sum(diag(confusionTab_rf))/sum(confusionTab_rf))*100,2)
acc_rf
```

```{r}
set.seed(1800)
trainctrl <- trainControl(method = "repeatedcv", number = 5, repeats = 3)

#Tuning grid
tuningGrid <- expand.grid(mtry = 1:15)

# Train the model
rfModel_tuned <- train(ADCDRSTG ~ ., data = train_data, method = "rf", trControl = trainctrl, tuneGrid = tuningGrid)
rfModel_tuned

plot(rfModel_tuned)
```
```{r}
# Optimized model
set.seed(1900)
optRf = randomForest(ADCDRSTG~., data = train_data, mtry = 14)
optPreds = predict(optRf, test_data)

plot(optRf)

# Calculate accuracy and misclassification
opt_rf_acc = sum(ifelse(optPreds==test_data$ADCDRSTG, 1, 0)) / nrow(test_data)
rf_tab = table(Predicted = optPreds, Actual = test_data$ADCDRSTG)

print(paste("The accuracy is", round(opt_rf_acc,4)))

misclass = 1-opt_rf_acc
print(paste("The misclassification error rate is", round(misclass,4)))
```
```{r}
#ROC for severe versus mild dementia
set.seed(580)
rf.fit = predict(optRf, newdata = test_data, type='class')
rf.fit = ifelse(as.integer(rf.fit) > 1, 1, 0)
test.fit = ifelse(as.integer(test_data$ADCDRSTG) > 1, 1, 0)


pred = prediction(rf.fit, test.fit)
perf = performance(pred, 'tpr','fpr')


# Calculate AUC
auc_roc = performance(pred, measure='auc')
auc_roc = auc_roc@y.values[[1]]

# Plot ROC
plot(perf, avg= "threshold",
     col='blue',
     main= "ROC Curve for Random Forest")
text(0.05, 1, paste0("AUC = ", round(auc_roc, 3)))
abline(0, 1, lty = 2, col = 'black')
legend(0.7,0.2, legend=c("ROC Curve", "Random Classifier"),
       col=c("blue", "black"), lty=1:2, cex=0.8)
```
```{r}
# Calculate precision, recall, f score
precision_rf <- diag(rf_tab) / colSums(rf_tab)
recall_rf <- diag(rf_tab) / rowSums(rf_tab)

f_scorerf <- 2 * (precision_rf * recall_rf) / (precision_rf + recall_rf)

precision_rf
recall_rf
f_scorerf

```

#Multonomial Logistic Model (RF variables)
```{r}
# Multinomial Logistic Regression (https://www.r-bloggers.com/2020/05/multinomial-logistic-regression-with-r/)
set.seed(580)
multinom_model <- nnet::multinom(formula = ADCDRSTG~ Memory + Wechsler_logical_memory + Fuld_object_memory + Del_word_list_memory + 
                                   Benton_vis_reten + Arithmetic + Learning, data = train_data)


#summary(multinom_model)

exp(coef(multinom_model)) # convert coefficients to odds
head(round(fitted(multinom_model), 2))

# Predicting the values for train dataset
multiMod <- predict(multinom_model, newdata = train_data, "class")

tab_multinom <- table(Predicted = multiMod,
                      Actual = train_data$ADCDRSTG)

tab_multinom

# Calculating accuracy - sum of diagonal elements divided by total obs
acc_multimod = round((sum(diag(tab_multinom))/sum(tab_multinom))*100,2)
print(paste("The accuracy of multinomial logistic regression (train) is", acc_multimod))

# Predicting the values for test dataset
multiMod_test <- predict(multinom_model, newdata = test_data, "class")

tab_multinom_test <- table(Predicted = multiMod_test,
                      Actual = test_data$ADCDRSTG)

tab_multinom_test

# Calculating accuracy - sum of diagonal elements divided by total obs
acc_multimod_test = round((sum(diag(tab_multinom_test))/sum(tab_multinom_test))*100,2)
print(paste("The accuracy of multinomial logistic regression (test) is", acc_multimod_test))

```
```{r}
#ROC for severe versus mild dementia
set.seed(580)
log.fit = predict(multinom_model, newdata = test_data, type='class')
log.fit = ifelse(as.integer(log.fit) > 1, 1, 0)
test.fit = ifelse(as.integer(test_data$ADCDRSTG) > 1, 1, 0)


pred = prediction(log.fit, test.fit)
perf = performance(pred, 'tpr','fpr')


# Calculate AUC
auc_roc = performance(pred, measure='auc')
auc_roc = auc_roc@y.values[[1]]

# Plot ROC
plot(perf, avg= "threshold",
     col='blue',
     main= "ROC Curve for Multinomial Logistic Regression")
text(0.05, 1, paste0("AUC = ", round(auc_roc, 3)))
abline(0, 1, lty = 2, col = 'black')
legend(0.7,0.2, legend=c("ROC Curve", "Random Classifier"),
       col=c("blue", "black"), lty=1:2, cex=0.8)
```
```{r}
# Calculate F-score for multonomial logistc regression (RF)
precision <- diag(tab_multinom_test) / colSums(tab_multinom_test)
recall <- diag(tab_multinom_test) / rowSums(tab_multinom_test)

f_score <- 2 * (precision * recall) / (precision + recall)
f_score

```

```{r}
# Calculate AIC BIC
aic_mlr_rf = AIC(multinom_model)
bic_mlr_rf = BIC(multinom_model)

aic_mlr_rf
bic_mlr_rf
```

```{r}
# Using PCs in multinom
set.seed(580)
pcs <- as.data.frame(pca$x)
pcs_x = pcs[,1:16]
y = train_data$ADCDRSTG

# Combine PCA loadings with outcome variable in dataframe pcaData (for training data)
pcs_x = pcs[,1:16]
pcaData <- cbind(y, pcs_x)

# Combine PCA loadings with outcome variable in dataframe pcaData (for test data)
pca_test <- prcomp(test_data[, 1:37], center=TRUE, scale=TRUE)
pcs_test <- as.data.frame(pca_test$x)
pcs_test_x = pcs_test[,1:16]
y_test = test_data$ADCDRSTG

pcaData <- cbind(y, pcs_x)
pcaData_test <- cbind(y_test, pcs_test_x)

multinom_model_pc <- nnet::multinom(formula = y ~ PC1 + PC2 + PC3 + PC4 + PC5 + PC6 + PC7 + PC8 
                                 + PC9 + PC10 + PC11 + PC12 + PC13 + + PC14 + PC15 + PC16, data = pcaData)

```


# Multonomial Logistic Regression using PCs
```{r}
set.seed(580)
exp(coef(multinom_model_pc)) # convert coefficients to odds
head(round(fitted(multinom_model_pc), 2))

# Predicting the values for train dataset
multiMod_pc <- predict(multinom_model_pc, newdata = pcaData, "class")

tab_multinom_pc <- table(Predicted = multiMod_pc,
                      Actual = pcaData$y)

tab_multinom_pc

# Calculating accuracy - sum of diagonal elements divided by total obs
acc_multimod_pc = round((sum(diag(tab_multinom_pc))/sum(tab_multinom_pc))*100,2)
print(paste("The accuracy of multinomial logistic regression using PCs (train) is", acc_multimod_pc))

# Predicting the values for test dataset
multiMod_test_pc <- predict(multinom_model_pc, newdata = pcaData_test, "class")

tab_multinom_test_pc <- table(Predicted = multiMod_test_pc,
                      Actual = pcaData_test$y_test)

tab_multinom_test_pc

# Calculating accuracy - sum of diagonal elements divided by total obs
acc_multimod_test_pc = round((sum(diag(tab_multinom_test_pc))/sum(tab_multinom_test_pc))*100,2)
print(paste("The accuracy of multinomial logistic regression using PCs (test) is", acc_multimod_test_pc))


```

```{r}
#ROC for severe versus mild dementia
set.seed(580)
log.fit = predict(multinom_model_pc, newdata = pcaData_test, type='class')
log.fit = ifelse(as.integer(log.fit) > 1, 1, 0)
test.fit = ifelse(as.integer(pcaData_test$y) > 1, 1, 0)


pred = prediction(log.fit, test.fit)
perf = performance(pred, 'tpr','fpr')


# Calculate AUC
auc_roc = performance(pred, measure='auc')
auc_roc = auc_roc@y.values[[1]]

# Plot ROC
plot(perf, avg= "threshold",
     col='blue',
     main= "ROC Curve for Multinomial Logistic Regression (PCs")
text(0.05, 1, paste0("AUC = ", round(auc_roc, 3)))
abline(0, 1, lty = 2, col = 'black')
legend(0.7,0.2, legend=c("ROC Curve", "Random Classifier"),
       col=c("blue", "black"), lty=1:2, cex=0.8)
```

#Linear Discrimminant Analysis
```{r}
set.seed(580)
ldaMod = lda(ADCDRSTG~., data = train_data)
ldaMod

# create confusion matrix for training set
ldaPred = predict(ldaMod, train_data)
confusionTab_LDA_train = table(Predicted = ldaPred$class,
                      Actual = train_data$ADCDRSTG)
confusionTab_LDA_train
acc_LDA = round((sum(diag(confusionTab_LDA_train))/sum(confusionTab_LDA_train))*100,2)
print(paste("The accuracy of LDA (train) is", acc_LDA))

# create confusion matrix for test set
ldaPred_test = predict(ldaMod, test_data)
confusionTab_LDA_test = table(Predicted = ldaPred_test$class,
                      Actual = test_data$ADCDRSTG)
confusionTab_LDA_test

# Calculating accuracy - sum of diagonal elements divided by total obs
acc_LDA_test = round((sum(diag(confusionTab_LDA_test))/sum(confusionTab_LDA_test))*100,2)
print(paste("The accuracy of LDA (test) is", acc_LDA_test))

#ROC for severe versus mild dementia
lda.fit = predict(ldaMod, newdata = test_data, type='response')
lda.fit = ifelse(as.integer(lda.fit$class) > 1, 1, 0)
test.fit = ifelse(as.integer(test_data$ADCDRSTG) > 1, 1, 0)


pred = prediction(lda.fit, test.fit)
perf = performance(pred, 'tpr','fpr')


# Calculate AUC
auc_roc = performance(pred, measure='auc')
auc_roc = auc_roc@y.values[[1]]

# Plot ROC
plot(perf, avg= "threshold",
     col='blue',
     main= "ROC Curve for LDA (Severe versus mild dementia)")
text(0.05, 1, paste0("AUC = ", round(auc_roc, 3)))
abline(0, 1, lty = 2, col = 'black')
legend(0.7,0.2, legend=c("ROC Curve", "Random Classifier"),
       col=c("blue", "black"), lty=1:2, cex=0.8)
```


```{r}
# Calculate F-score for LDA
precision <- diag(confusionTab_LDA_test) / colSums(confusionTab_LDA_test)
recall <- diag(confusionTab_LDA_test) / rowSums(confusionTab_LDA_test)

f_score <- 2 * (precision * recall) / (precision + recall)
f_score
```


#KNN
```{r}
set.seed(580)
for (i in seq(15, 40 , 5)) {
  knn = knn(train_data, test_data, cl = train_data$ADCDRSTG, k = i)
  tab = table(knn,test_data$ADCDRSTG)
  print(paste("k =",i))
  print(tab)
  
  accuracy = round((sum(diag(tab))/sum(tab))*100,2)
  print(paste("Accuracy:", accuracy))
} # Chose the optimal model to be k=25
```
```{r}
#ROC for severe versus mild dementia
knn.fit = knn(train_data, test_data, cl = train_data$ADCDRSTG, k = 25)
knn.fit = ifelse(as.integer(knn.fit) > 1, 1, 0)
test.fit = ifelse(as.integer(test_data$ADCDRSTG) > 1, 1, 0)


pred = prediction(knn.fit, test.fit)
perf = performance(pred, measure = "tpr", x.measure = "fpr")


# Calculate AUC
auc_roc = performance(pred, measure='auc')
auc_roc = auc_roc@y.values[[1]]

# Plot ROC
plot(perf, avg= "threshold",
     col='blue',
     main= "ROC Curve for KNN")
text(0.05, 1, paste0("AUC = ", round(auc_roc, 3)))
abline(0, 1, lty = 2, col = 'black')
legend(0.7,0.2, legend=c("ROC Curve", "Random Classifier"),
       col=c("blue", "black"), lty=1:2, cex=0.8)
```

# Polynomial Logistic Regression 
```{r}
# Polynomial Logistic Model with variables chosen using RF
set.seed(580)
polyLogModel <- nnet::multinom(ADCDRSTG ~ poly(Wechsler_logical_memory, 2) + poly(Fuld_object_memory, 2) + 
                         poly(Memory, 2) + poly(Benton_vis_reten, 2) + poly(Del_word_list_memory,2)
                         + poly(Arithmetic,2) + poly(Learning,2),
                         data = train_data, raw = FALSE)

# create confusion matrix for test set
polyPred_test = predict(polyLogModel, test_data)
confusionTab_poly_test = table(Predicted = polyPred_test,
                      Actual = test_data$ADCDRSTG)
confusionTab_poly_test

# Calculating accuracy - sum of diagonal elements divided by total obs
acc_poly_test = round((sum(diag(confusionTab_poly_test))/sum(confusionTab_poly_test))*100,2)
print(paste("The accuracy of polynomial logistic regression using RF (test) is", acc_poly_test))


#ROC for severe versus mild dementia
polylog.fit = predict(polyLogModel, newdata = test_data, type='class')
polylog.fit = ifelse(as.integer(polylog.fit) > 1, 1, 0)
test.fit = ifelse(as.integer(test_data$ADCDRSTG) > 1, 1, 0)


pred = prediction(polylog.fit, test.fit)
perf = performance(pred, measure = "tpr", x.measure = "fpr")


# Calculate AUC
auc_roc = performance(pred, measure='auc')
auc_roc = auc_roc@y.values[[1]]

# Plot ROC
plot(perf, avg= "threshold",
     col='blue',
     main= "ROC Curve for Polynomial Logistic Regression")
text(0.05, 1, paste0("AUC = ", round(auc_roc, 3)))
abline(0, 1, lty = 2, col = 'black')
legend(0.7,0.2, legend=c("ROC Curve", "Random Classifier"),
       col=c("blue", "black"), lty=1:2, cex=0.8)
```

```{r}
# Calculate precision, recall, f score
precision_poly <- diag(confusionTab_poly_test) / colSums(confusionTab_poly_test)
recall_poly <- diag(confusionTab_poly_test) / rowSums(confusionTab_poly_test)

f_scorepoly <- 2 * (precision_poly * recall_poly) / (precision_poly + recall_poly)

precision_poly
recall_poly
f_scorepoly
```

```{r}
# Calculate AIC BIC for Poly RF
poly.aic = AIC(polyLogModel)
poly.aic

poly.bic = BIC(polyLogModel)
poly.bic

```

# Classification Decision Tree
```{r}
set.seed(580)
treeMod <- rpart(ADCDRSTG ~ ., data = train_data, method = "class")

# Plot decision tree
rpart.plot(treeMod, type = 4)

predict_tree <- predict(treeMod, newdata = test_data, type = "class")

# Calculate accuracy
predict_dt = factor(predict_tree,levels = c(0,0.5,1,2,3))
tab_dt <- confusionMatrix(predict_dt, 
                        test_data$ADCDRSTG)
tab_dt

# create confusion matrix for test set
dtPred_test = predict(treeMod, test_data,"class")
confusionTab_dt_test = table(Predicted = dtPred_test,
                      Actual = test_data$ADCDRSTG)
confusionTab_dt_test

# Calculating accuracy - sum of diagonal elements divided by total obs
acc_dt_test = round((sum(diag(confusionTab_dt_test))/sum(confusionTab_dt_test))*100,2)
print(paste("The accuracy of decision tree (test) is", acc_dt_test))


# Overall performance stats of classification decision tree model
predict_tree = factor(predict_tree,levels = c(0,0.5,1,2,3))
tab_tree <- confusionMatrix(predict_tree, test_data$ADCDRSTG)
tab_tree

#ROC for severe versus mild dementia
ct.fit = predict(treeMod, newdata = test_data, type='class')
ct.fit = ifelse(as.integer(ct.fit) > 1, 1, 0)
test.fit = ifelse(as.integer(test_data$ADCDRSTG) > 1, 1, 0)


pred = prediction(ct.fit, test.fit)
perf = performance(pred, 'tpr','fpr')


# Calculate AUC
auc_roc = performance(pred, measure='auc')
auc_roc = auc_roc@y.values[[1]]

# Plot ROC
plot(perf, avg= "threshold",
     col='blue',
     main= "ROC Curve for Classification Tree (Severe versus mild dementia)")
text(0.05, 1, paste0("AUC = ", round(auc_roc, 3)))
abline(0, 1, lty = 2, col = 'black')
legend(0.7,0.2, legend=c("ROC Curve", "Random Classifier"),
       col=c("blue", "black"), lty=1:2, cex=0.8)


```

```{r}
#Prune Tree
set.seed(580)
treeMod_prune <- rpart(ADCDRSTG ~ ., data = train_data, method = "class", control = rpart.control(cp=0.03, 
                                                                                                  minsplit = 10,
                                                                                                  minbucket = round(10/3),
                                                                                                  maxdepth = 10))

# Plot decision tree
rpart.plot(treeMod_prune, type = 4)

predict_tree_prune <- predict(treeMod_prune, newdata = test_data, type = "class")

# Calculate accuracy
predict_dt <- predict(treeMod_prune, newdata = test_data, "class")

tab_dt <- table(Predicted = predict_dt,
                      Actual = test_data$ADCDRSTG)
acc = round((sum(diag(tab_dt))/sum(tab_dt))*100,3)


# Overall performance stats of classification decision tree model
predict_tree_prune = factor(predict_tree_prune,levels = c(0,0.5,1,2,3))
tab_tree_prune <- confusionMatrix(predict_tree_prune, test_data$ADCDRSTG)
tab_tree_prune

#ROC for severe versus mild dementia
ct.fit.prune = predict(treeMod_prune, newdata = test_data, type='class')
ct.fit.prune = ifelse(as.integer(ct.fit.prune) > 1, 1, 0)
test.fit.prune = ifelse(as.integer(test_data$ADCDRSTG) > 1, 1, 0)


pred_prune = prediction(ct.fit.prune, test.fit.prune)
perf_prune = performance(pred_prune, 'tpr','fpr')


# Calculate AUC
auc_roc = performance(pred_prune, measure='auc')
auc_roc = auc_roc@y.values[[1]]

# Plot ROC
plot(perf_prune, avg= "threshold",
     col='blue',
     main= "ROC Curve for Classification Tree (Severe versus mild dementia)")
text(0.05, 1, paste0("AUC = ", round(auc_roc, 3)))
abline(0, 1, lty = 2, col = 'black')
legend(0.7,0.2, legend=c("ROC Curve", "Random Classifier"),
       col=c("blue", "black"), lty=1:2, cex=0.8)
```

# SVM
```{r}
# SVM with Radial
# Scale dataframe
set.seed(580)
scaled_train = train_data %>% mutate(across(-ADCDRSTG, scale))
scaled_test = test_data %>% mutate(across(-ADCDRSTG, scale)) 

# Train SVM Model on scaled train data
svmMod = svm(ADCDRSTG~., data = scaled_train, type = 'C', cost = 50,
             kernel =  'radial')

#plot(x=svmMod, data = scaled_train, formula = MMSE_score~Organization)

costVal = trainControl(method = 'repeatedcv',
                       repeats = 10,
                       number = 10)

optParamGrid = expand.grid(C=c(.001, .01, 1, 5, 10, 20, 50, 100),
                                            sigma = c(0.5, 1, 2, 3, 4))

svmCv = train(ADCDRSTG~., 
              data = scaled_train,
              method = 'svmRadial',
              trControl = costVal,
              tuneGrid = optParamGrid,
              metric = 'Kappa')

optimal_cost = svmCv$bestTune

originalSVMPred = predict(svmMod, scaled_test, type = 'class')
optSVMPred = predict(svmCv, scaled_test, type = 'raw')


CrossTable(originalSVMPred, scaled_test$ADCDRSTG)
CrossTable(optSVMPred, scaled_test$ADCDRSTG)

# Calculate accuracy
tab = table(optSVMPred,scaled_test$ADCDRSTG)
tab

accuracy = round((sum(diag(tab))/sum(tab))*100,3)
print(paste("The accuracy for SVM is", accuracy))

```


```{r}
set.seed(580)
# Calculate accuracy
predict_svm <- predict(svmMod, scaled_test, type = 'class')

tab_svm <- table(Predicted = predict_svm,
                      Actual = scaled_test$ADCDRSTG)
acc = round((sum(diag(tab_svm))/sum(tab_svm))*100,2)


#ROC for severe versus mild dementia
svm.fit = predict(svmMod, newdata = scaled_test, type = 'class')
svm.fit = ifelse(as.integer(svm.fit) > 1, 1, 0)
test.fit = ifelse(as.integer(scaled_test$ADCDRSTG) > 1, 1, 0)


pred_svm = prediction(svm.fit, test.fit)
perf_svm = performance(pred_svm, 'tpr','fpr')


# Calculate AUC
auc_roc = performance(pred_svm, measure='auc')
auc_roc = auc_roc@y.values[[1]]

# Plot ROC
plot(perf_svm, avg= "threshold",
     col='blue',
     main= "ROC Curve for SVM (Severe versus mild dementia)")
text(0.05, 1, paste0("AUC = ", round(auc_roc, 3)))
abline(0, 1, lty = 2, col = 'black')
legend(0.7,0.2, legend=c("ROC Curve", "Random Classifier"),
       col=c("blue", "black"), lty=1:2, cex=0.8)
```
```{r}
# Calculate precision, recall, f score
precision_svm <- diag(tab_svm) / colSums(tab_svm)
recall_svm <- diag(tab_svm) / rowSums(tab_svm)

f_scoresvm <- 2 * (precision_svm * recall_svm) / (precision_svm + recall_svm)

precision_svm
recall_svm
f_scoresvm

```


```{r}
# Train SVM Model on PCs
svmMod = svm(y~., data = pcaData, type = 'C', cost = 50,
             kernel =  'polynomial')

#plot(x=svmMod, data = scaled_train, formula = MMSE_score~Organization)

costVal = trainControl(method = 'repeatedcv',
                       repeats = 3,
                       number = 10)
svmCv = train(y~., 
              data = pcaData,
              method = 'svmLinear',
              trControl = costVal,
              tuneGrid = expand.grid(C=c(10^seq(from = -3, to = 2, by = .2))),
              metric = 'Kappa')

optimal_cost = svmCv$bestTune

originalSVMPred = predict(svmMod, pcaData_test, type = 'class')
optSVMPred = predict(svmCv, pcaData_test, type = 'raw')


CrossTable(originalSVMPred, pcaData_test$y)
CrossTable(optSVMPred, pcaData_test$y)

optParamGrid = expand.grid(C=c(.001, .01, 1, 5, 10, 20, 50, 100),
                                            sigma = c(0.5, 1, 2, 3, 4))

optRadSVM = train(y~.,
                  pcaData,
                  method = 'svmLinear',
                  tuneGrid = expand.grid(C=c(10^seq(from = -3, to = 2, by = .2))),
                  trControl = costVal,
                  metric = 'Accuracy') 
print(optRadSVM)
optRadSVM$bestTune

optRadPred = predict(optRadSVM, pcaData_test, type = 'raw')
CrossTable(optRadPred, pcaData_test$y)

# Calculate accuracy
tab = table(optRadPred,pcaData_test$y)
tab

accuracy = round((sum(diag(tab))/sum(tab))*100,3)
print(paste("The accuracy for SVM (pca) is", accuracy))
```

```{r}
# SVM with Linear
set.seed(580)

# Train SVM Model on scaled train data
svmMod = svm(ADCDRSTG~., data = scaled_train, type = 'C', cost = 50,
             kernel =  'linear')

costVal = trainControl(method = 'repeatedcv',
                       repeats = 10,
                       number = 10)


svmCv = train(ADCDRSTG~., 
              data = scaled_train,
              method = 'svmLinear',
              trControl = costVal,
              tuneGrid = expand.grid(C=c(10^seq(from = -3, to = 2, by = .2))),
              metric = 'Kappa')

optimal_cost = svmCv$bestTune

originalSVMPred = predict(svmMod, scaled_test, type = 'class')
optSVMPred = predict(svmCv, scaled_test, type = 'raw')


CrossTable(originalSVMPred, scaled_test$ADCDRSTG)
CrossTable(optSVMPred, scaled_test$ADCDRSTG)

# Calculate accuracy
tab = table(optSVMPred,scaled_test$ADCDRSTG)
tab

accuracy = round((sum(diag(tab))/sum(tab))*100,3)
print(paste("The accuracy for SVM is", accuracy))

```


```{r}
set.seed(580)
# Calculate accuracy
predict_svm <- predict(svmMod, scaled_test, type = 'class')

tab_svm <- table(Predicted = predict_svm,
                      Actual = scaled_test$ADCDRSTG)
acc = round((sum(diag(tab_svm))/sum(tab_svm))*100,2)


#ROC for severe versus mild dementia
svm.fit = predict(svmMod, newdata = scaled_test, type = 'class')
svm.fit = ifelse(as.integer(svm.fit) > 1, 1, 0)
test.fit = ifelse(as.integer(scaled_test$ADCDRSTG) > 1, 1, 0)


pred_svm = prediction(svm.fit, test.fit)
perf_svm = performance(pred_svm, 'tpr','fpr')


# Calculate AUC
auc_roc = performance(pred_svm, measure='auc')
auc_roc = auc_roc@y.values[[1]]

# Plot ROC
plot(perf_svm, avg= "threshold",
     col='blue',
     main= "ROC Curve for SVM (Severe versus mild dementia)")
text(0.05, 1, paste0("AUC = ", round(auc_roc, 3)))
abline(0, 1, lty = 2, col = 'black')
legend(0.7,0.2, legend=c("ROC Curve", "Random Classifier"),
       col=c("blue", "black"), lty=1:2, cex=0.8)
```
```{r}
# SVM with Polynomial
set.seed(580)

# Train SVM Model on scaled train data
svmMod = svm(ADCDRSTG~., data = scaled_train, type = 'C', cost = 50,
             kernel =  'polynomial', degree = 3)

costVal = trainControl(method = 'repeatedcv',
                       repeats = 10,
                       number = 10)

#optParamGrid = expand.grid(C=c(.001, .01, 1, 5, 10, 20, 50, 100),
                                            #sigma = c(0.5, 1, 2, 3, 4))

svmCv = train(ADCDRSTG~., 
              data = scaled_train,
              method = 'svmPoly',
              trControl = costVal,
              #tuneGrid = optParamGrid,
              metric = 'Kappa')

optimal_cost = svmCv$bestTune

originalSVMPred = predict(svmMod, scaled_test, type = 'class')
optSVMPred = predict(svmCv, scaled_test, type = 'raw')


CrossTable(originalSVMPred, scaled_test$ADCDRSTG)
CrossTable(optSVMPred, scaled_test$ADCDRSTG)

# Calculate accuracy
tab = table(optSVMPred,scaled_test$ADCDRSTG)
tab

accuracy = round((sum(diag(tab))/sum(tab))*100,3)
print(paste("The accuracy for SVM is", accuracy))

```
# Stepwise / Ridge to enhance model features
```{r} 
########################################### 
#Using StepwiseAIC to select the variables#
########################################### 
set.seed(580)

# Using multinomial logistic regression (multinom_model)
fit1 <- nnet::multinom(formula = ADCDRSTG~ Memory + Wechsler_logical_memory + Fuld_object_memory + Del_word_list_memory + 
                                   Benton_vis_reten + Arithmetic + Learning, data = train_data)

 step.fit1 <- stepAIC(fit1, direction = 'both', trace = FALSE)
step.fit1$aic
# summary(step.fit1)

# Prediction on test set
predict_aic_multi = predict(step.fit1, newdata = test_data)
confusionTab_aic_multi_test = table(Predicted = predict_aic_multi,
                      Actual = test_data$ADCDRSTG)
confusionTab_aic_multi_test

acc_aic_multi = round((sum(diag(confusionTab_aic_multi_test))/sum(confusionTab_aic_multi_test))*100,2)
print(paste("The accuracy of Multinomial Logistic Regression from stepwise is", acc_aic_multi))
```

```{r}
# Polynomial Logistic Regression
set.seed(580)
fit2 <- nnet::multinom(ADCDRSTG ~ poly(Wechsler_logical_memory, 2) + poly(Fuld_object_memory, 2) + 
                         poly(Memory, 2) + poly(Benton_vis_reten, 2) + poly(Del_word_list_memory,2)
                         + poly(Arithmetic,2) + poly(Learning,2),
                         data = train_data, raw = FALSE)

step.fit2 <- stepAIC(fit2, direction = 'both', trace = FALSE)
step.fit2$aic
summary(step.fit2)

predict_aic_poly = predict(step.fit2, newdata = test_data)
confusionTab_aic_poly_test = table(Predicted = predict_aic_poly,
                      Actual = test_data$ADCDRSTG)
confusionTab_aic_poly_test

acc_aic_poly = round((sum(diag(confusionTab_aic_poly_test))/sum(confusionTab_aic_poly_test))*100,2)
print(paste("The accuracy of Polynomial Logistic Regression from stepwise is", acc_aic_poly))

```

# Section 3: Model Evaluation Criteria 

```{r}
# Polynomial Logistic Regression - based on best RF variables
plr_aic = AIC(polyLogModel)
plr_bic = BIC(polyLogModel)

plr_aic
plr_bic
```

```{r}
# Multinomial Logistic Regression (RF)
mlr_aic = AIC(multinom_model)
mlr_bic = BIC(multinom_model)

mlr_aic
mlr_bic
```

```{r}
# Multinomial Logistic Regression (PC)
mlr_aic = AIC(multinom_model)
mlr_bic = BIC(multinom_model)
```

